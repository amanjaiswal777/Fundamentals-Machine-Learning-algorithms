{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression\n",
        "\n",
        "Linear regression is a statistical method used to model the relationship between a dependent variable (also called the response variable) and one or more independent variables (also called predictor variables). The basic idea is to find the best-fit line or plane that can predict the dependent variable based on the independent variables.\n",
        "\n",
        "The linear regression equation for a simple linear regression model with one independent variable is given by:\n",
        "\n",
        "Y = b0 + b1*X + e\n",
        "\n",
        "where:\n",
        "\n",
        "Y is the dependent variable (response variable)\n",
        "X is the independent variable (predictor variable)\n",
        "b0 is the intercept or constant term\n",
        "b1 is the slope coefficient (the change in Y for a unit change in X)\n",
        "e is the error term (the difference between the predicted and actual values of Y)\n",
        "The goal of linear regression is to estimate the values of the intercept and slope coefficients (b0 and b1) that minimize the sum of squared errors between the predicted and actual values of Y. This is typically done using the method of least squares, which involves finding the values of b0 and b1 that minimize the sum of the squared residuals:\n",
        "\n",
        "Sum of squared residuals = Σ (Yi - Ŷi)2\n",
        "\n",
        "where:\n",
        "\n",
        "Yi is the actual value of the dependent variable for the i-th observation\n",
        "Ŷi is the predicted value of the dependent variable for the i-th observation\n",
        "The solution to the least squares problem can be obtained using matrix algebra. Specifically, the solution is given by:\n",
        "\n",
        "B = (X^T X)^-1 X^T Y\n",
        "\n",
        "where:\n",
        "\n",
        "B is a vector of the estimated coefficients (b0 and b1)\n",
        "X is the matrix of independent variables (including a column of ones for the intercept)\n",
        "Y is the vector of dependent variable values\n",
        "Once the coefficients are estimated, we can use the linear regression equation to make predictions on new data.\n",
        "\n",
        "Linear regression can also be extended to multiple linear regression, where there are multiple independent variables. The equation is similar, but there is a coefficient for each independent variable:\n",
        "\n",
        "Y = b0 + b1X1 + b2X2 + ... + bn*Xn + e\n",
        "\n",
        "where:\n",
        "\n",
        "X1, X2, ..., Xn are the independent variables\n",
        "b1, b2, ..., bn are the slope coefficients for each independent variable\n",
        "The solution to the multiple linear regression problem involves estimating the values of the coefficients that minimize the sum of squared errors, using techniques such as matrix algebra or gradient descent."
      ],
      "metadata": {
        "id": "j1r40fzjUWu_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yk0hSFIgT8x6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self, lr=0.01, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        # Initialize weights and bias\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        \n",
        "        # Gradient descent\n",
        "        for _ in range(self.n_iters):\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "        \n",
        "    def predict(self, X):\n",
        "        y_pred = np.dot(X, self.weights) + self.bias\n",
        "        return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a toy dataset\n",
        "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "y = np.array([6, 15, 24])\n",
        "\n",
        "# Initialize the linear regression model\n",
        "model = LinearRegression(lr=0.01, n_iters=1000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions on new data\n",
        "X_test = np.array([[10, 11, 12], [13, 14, 15]])\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HNj1IszUaGs",
        "outputId": "1648d432-5156-4388-a375-2675c4d8fd45"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32.99872173 41.99792905]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Logistic regression is a type of classification algorithm that is used to model the relationship between a binary (or categorical) output variable and one or more input variables. It is called logistic regression because it uses a logistic (or sigmoid) function to model the probability of the output variable taking on a particular value, given the input variables.\n",
        "\n",
        "The logistic function is defined as:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "�\n",
        "−\n",
        "�\n",
        "σ(z)= \n",
        "1+e \n",
        "−z\n",
        " \n",
        "1\n",
        "​\n",
        " \n",
        "\n",
        "where $z = w_0 + w_1x_1 + w_2x_2 + ... + w_mx_m$ is a linear combination of the input variables and their associated weights.\n",
        "\n",
        "The logistic function maps any real-valued number $z$ to a value between 0 and 1, which can be interpreted as the probability of the output variable being 1, given the input variables. For example, if $\\sigma(z) = 0.8$, then we can say that the probability of the output variable being 1 is 0.8.\n",
        "\n",
        "To learn the weights $w_0, w_1, w_2, ..., w_m$, we need to define a cost function that measures how well the logistic regression model is doing at predicting the output variable. A common cost function for logistic regression is the cross-entropy loss function, which is defined as:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "�\n",
        "∑\n",
        "�\n",
        "=\n",
        "1\n",
        "�\n",
        "[\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "�\n",
        "(\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "�\n",
        "(\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "J(w)=− \n",
        "m\n",
        "1\n",
        "​\n",
        " ∑ \n",
        "i=1\n",
        "m\n",
        "​\n",
        " [y \n",
        "(i)\n",
        " log(σ(z \n",
        "(i)\n",
        " ))+(1−y \n",
        "(i)\n",
        " )log(1−σ(z \n",
        "(i)\n",
        " ))]\n",
        "\n",
        "where $m$ is the number of training examples, $y^{(i)}$ is the actual output value for the $i$th training example, and $z^{(i)}$ is the linear combination of input variables and weights for the $i$th training example. The goal is to minimize the value of the cost function $J(w)$ with respect to the weights $w_0, w_1, w_2, ..., w_m$.\n",
        "\n",
        "To minimize the cost function, we can use an optimization algorithm such as gradient descent. The gradient of the cost function with respect to the weights is given by:\n",
        "\n",
        "∂\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "∂\n",
        "�\n",
        "�\n",
        "=\n",
        "1\n",
        "�\n",
        "∑\n",
        "�\n",
        "=\n",
        "1\n",
        "�\n",
        "(\n",
        "�\n",
        "(\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        ")\n",
        "−\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        ")\n",
        "�\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "∂w \n",
        "j\n",
        "​\n",
        " \n",
        "∂J(w)\n",
        "​\n",
        " = \n",
        "m\n",
        "1\n",
        "​\n",
        " ∑ \n",
        "i=1\n",
        "m\n",
        "​\n",
        " (σ(z \n",
        "(i)\n",
        " )−y \n",
        "(i)\n",
        " )x \n",
        "j\n",
        "(i)\n",
        "​\n",
        " \n",
        "\n",
        "where $x_j^{(i)}$ is the $j$th input variable for the $i$th training example. We can use this gradient to update the weights using the following update rule:\n",
        "\n",
        "�\n",
        "�\n",
        ":\n",
        "=\n",
        "�\n",
        "�\n",
        "−\n",
        "�\n",
        "∂\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "∂\n",
        "�\n",
        "�\n",
        "w \n",
        "j\n",
        "​\n",
        " :=w \n",
        "j\n",
        "​\n",
        " −α \n",
        "∂w \n",
        "j\n",
        "​\n",
        " \n",
        "∂J(w)\n",
        "​\n",
        " \n",
        "\n",
        "where $\\alpha$ is the learning rate.\n",
        "\n",
        "By repeating this update rule iteratively, we can find the weights that minimize the cost function and provide us with the best logistic regression model for our data."
      ],
      "metadata": {
        "id": "RDArQbRiV8GE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
        "        self.lr = lr\n",
        "        self.num_iter = num_iter\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.verbose = verbose\n",
        "    \n",
        "    def __add_intercept(self, X):\n",
        "        intercept = np.ones((X.shape[0], 1))\n",
        "        return np.concatenate((intercept, X), axis=1)\n",
        "    \n",
        "    def __sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    def __loss(self, h, y):\n",
        "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "        \n",
        "        # initialize weights\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        \n",
        "        for i in range(self.num_iter):\n",
        "            z = np.dot(X, self.theta)\n",
        "            h = self.__sigmoid(z)\n",
        "            gradient = np.dot(X.T, (h - y)) / y.size\n",
        "            self.theta -= self.lr * gradient\n",
        "            \n",
        "            if self.verbose and i % 10000 == 0:\n",
        "                z = np.dot(X, self.theta)\n",
        "                h = self.__sigmoid(z)\n",
        "                print(f'Loss: {self.__loss(h, y)}')\n",
        "    \n",
        "    def predict_prob(self, X):\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "        \n",
        "        return self.__sigmoid(np.dot(X, self.theta))\n",
        "    \n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return self.predict_prob(X) >= threshold\n"
      ],
      "metadata": {
        "id": "HBl86GEBUcv1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# generate some random data for classification\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# create a logistic regression model\n",
        "model = LogisticRegression(lr=0.1, num_iter=100000)\n",
        "\n",
        "# train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# calculate the accuracy of the predictions\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFRg23VNWGww",
        "outputId": "26d0f146-6cf1-497a-e77a-aa44e4cda5a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UBhuSgR7WKhK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}